{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complete-surname",
   "metadata": {},
   "source": [
    "<h2>Predicting Fuel Efficiency of Vehicles - Part 3 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-worse",
   "metadata": {},
   "source": [
    "<h3>Selecting and Training Models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-scotland",
   "metadata": {},
   "source": [
    "1. Select and Train a few Algorithms(Linear Regression, Decision Tree, RandomForest)\n",
    "2. Evaluation using Mean Squared Error\n",
    "3. Model Evaluation using Cross Validation\n",
    "4. Hyperparameter Tuning using GridSearchCV\n",
    "5. Check Feature Importance\n",
    "6. Evaluate the Final System on test data\n",
    "7. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convinced-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing a few general use case libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metric-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the .data file using pandas\n",
    "\n",
    "cols = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "df = pd.read_csv('./auto-mpg.data', names=cols, na_values = \"?\",\n",
    "                comment = '\\t',\n",
    "                sep= \" \",\n",
    "                skipinitialspace=True)\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"Cylinders\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "black-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>4</td>\n",
       "      <td>156.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2585.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3139.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2265.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2108.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>4668.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2930.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cylinders  Displacement  Horsepower  Weight  Acceleration  Model Year  \\\n",
       "145          4          83.0        61.0  2003.0          19.0          74   \n",
       "151          4          79.0        67.0  2000.0          16.0          74   \n",
       "388          4         156.0        92.0  2585.0          14.5          82   \n",
       "48           6         250.0        88.0  3139.0          14.5          71   \n",
       "114          4          98.0        90.0  2265.0          15.5          73   \n",
       "..         ...           ...         ...     ...           ...         ...   \n",
       "147          4          90.0        75.0  2108.0          15.5          74   \n",
       "156          8         400.0       170.0  4668.0          11.5          75   \n",
       "395          4         135.0        84.0  2295.0          11.6          82   \n",
       "14           4         113.0        95.0  2372.0          15.0          70   \n",
       "362          6         146.0       120.0  2930.0          13.8          81   \n",
       "\n",
       "     Origin  \n",
       "145       3  \n",
       "151       2  \n",
       "388       1  \n",
       "48        1  \n",
       "114       2  \n",
       "..      ...  \n",
       "147       2  \n",
       "156       1  \n",
       "395       1  \n",
       "14        3  \n",
       "362       3  \n",
       "\n",
       "[318 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##segregate the feature and target variable\n",
    "data = strat_train_set.drop(\"MPG\", axis=1)\n",
    "data_labels = strat_train_set[\"MPG\"].copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developed-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocess the Origin column in data  \n",
    "def preprocess_origin_cols(df):\n",
    "    df[\"Origin\"] = df[\"Origin\"].map({1: \"India\", 2: \"USA\", 3: \"Germany\"})  #renaming 1, 2, 3 for India, USA, Germany\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "robust-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating custom attribute adder class  (It works the same way like part 2)\n",
    "acc_ix, hpower_ix, cyl_ix = 4,2, 0\n",
    "\n",
    "class CustomAttrAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, acc_on_power=True): # no *args or **kargs\n",
    "        self.acc_on_power = acc_on_power\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        acc_on_cyl = X[:, acc_ix] / X[:, cyl_ix]\n",
    "        if self.acc_on_power:\n",
    "            acc_on_power = X[:, acc_ix] / X[:, hpower_ix]\n",
    "            return np.c_[X, acc_on_power, acc_on_cyl]\n",
    "        \n",
    "        return np.c_[X, acc_on_cyl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "disturbed-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_pipeline_transformer(data):\n",
    "    '''\n",
    "    Function to process numerical transformations\n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        num_attrs: numerical dataframe\n",
    "        num_pipeline: numerical pipeline object\n",
    "        \n",
    "    '''\n",
    "    numerics = ['float64', 'int64']\n",
    "\n",
    "    num_attrs = data.select_dtypes(include=numerics)\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attrs_adder', CustomAttrAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "    return num_attrs, num_pipeline\n",
    "\n",
    "\n",
    "def pipeline_transformer(data):\n",
    "    '''\n",
    "    Complete transformation pipeline for both\n",
    "    nuerical and categorical data.\n",
    "    \n",
    "    Argument:\n",
    "        data: original dataframe \n",
    "    Returns:\n",
    "        prepared_data: transformed data, ready to use\n",
    "    '''\n",
    "    cat_attrs = [\"Origin\"]\n",
    "    num_attrs, num_pipeline = num_pipeline_transformer(data)\n",
    "    #print(list(num_attrs)) I can delete this\n",
    "    full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, list(num_attrs)),\n",
    "        (\"cat\", OneHotEncoder(), cat_attrs),\n",
    "        ])\n",
    "    prepared_data = full_pipeline.fit_transform(data)\n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-twenty",
   "metadata": {},
   "source": [
    "<h3>From raw data to processed data in 2 steps</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "removable-alert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85657842, -1.07804475, -1.15192977, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.85657842, -1.1174582 , -0.9900351 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.85657842, -0.3587492 , -0.31547399, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.85657842, -0.56566984, -0.53133355, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.85657842, -0.78244384, -0.23452666, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.32260746, -0.45728283,  0.44003446, ...,  1.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##from raw data to processed data in 2 steps\n",
    "preprocessed_df = preprocess_origin_cols(data)\n",
    "prepared_data = pipeline_transformer(preprocessed_df)\n",
    "prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "paperback-remark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.85657842, -1.07804475, -1.15192977, -1.17220298,  1.21586943,\n",
       "       -0.54436373,  1.70952741,  1.29565517,  1.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data[0]  #the first rows of the prepared data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-twins",
   "metadata": {},
   "source": [
    "<h3>Selecting and Training Models</h3>\n",
    "1. Linear Regression <br>\n",
    "2. Decision Tree <br>\n",
    "3. Random Forest <br>\n",
    "4. SVM regressor <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "therapeutic-isolation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(prepared_data, data_labels)#it gives a linear regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "assumed-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of samples:  [29.08069379 27.78336755 26.08031176 12.70419279 22.23454159]\n"
     ]
    }
   ],
   "source": [
    "##testing the predictions\n",
    "sample_data = data.iloc[:5]  #5 rows for a sample data\n",
    "sample_labels = data_labels.iloc[:5]\n",
    "\n",
    "sample_data_prepared = pipeline_transformer(sample_data)\n",
    "\n",
    "print(\"Prediction of samples: \", lin_reg.predict(sample_data_prepared)) \n",
    "#if I want to print the name of the labels, activate #print(list(num_attrs)) in num_pipeline_transformer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interesting-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Labels of samples:  [32.0, 31.0, 26.0, 18.0, 26.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual Labels of samples: \", list(sample_labels))  #this is how linear regression perform:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-resort",
   "metadata": {},
   "source": [
    "<h4>Mean Squared Error</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "african-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9590402225760872"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mpg_predictions = lin_reg.predict(prepared_data)\n",
    "lin_mse = mean_squared_error(data_labels, mpg_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-cutting",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "framed-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(prepared_data, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "smart-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_predictions = tree_reg.predict(prepared_data)\n",
    "tree_mse = mean_squared_error(data_labels, mpg_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-oxygen",
   "metadata": {},
   "source": [
    "But no model is perfect, this means that our model has overfit the data to a great extent. (the model is highly overfitting the data, it fits well the training (seen) data but it performs poorly on unseen data which is has not seen yet)\n",
    "\n",
    "We won't be touching out test data until we finalize our model. So, how do we check for what's happening?\n",
    "\n",
    "Model Evaluation using Cross Validation\n",
    "Scikit-Learnâ€™s K-fold cross-validation feature randomly splits the training set into K distinct subsets called folds, \n",
    "then it trains and evaluates the model K times, picking a different fold for evaluation every time and training on the \n",
    "other K-1 folds.\n",
    "\n",
    "The result is an array containing the K evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "given-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg,   #this function will give me 10 scores\n",
    "                         prepared_data, \n",
    "                         data_labels, \n",
    "                         scoring=\"neg_mean_squared_error\",  #this is an argument (neg_mean_squared_error), it accepts an argument\n",
    "                         cv = 10) #10 cross validations or 10 folds\n",
    "tree_reg_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suffering-boxing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.1257999 , 3.23684839, 3.08043219, 3.36841951, 2.28992358,\n",
       "       2.99134167, 3.09652022, 4.39349661, 4.12928175, 2.71602176])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg_rmse_scores  #10 scores root mean square error values for the 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "overall-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.242808558763806"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg_rmse_scores.mean()  #the mean of the 10 scores Result: 3.2428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "executed-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.43254597, 3.45157629, 3.6621715 , 2.59652976, 2.48023405,\n",
       "       2.74798115, 3.32524647, 2.42208917, 3.78133275, 2.8573747 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lin_reg, prepared_data, data_labels, scoring=\"neg_mean_squared_error\", cv = 10)\n",
    "lin_reg_rmse_scores = np.sqrt(-scores)\n",
    "lin_reg_rmse_scores   #the same for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "soviet-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0757081793709324"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_rmse_scores.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-fantasy",
   "metadata": {},
   "source": [
    "<h3>Random Forest model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pending-brush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5585913052822704"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(prepared_data, data_labels)\n",
    "forest_reg_cv_scores = cross_val_score(forest_reg,\n",
    "                                         prepared_data,\n",
    "                                         data_labels,\n",
    "                                         scoring='neg_mean_squared_error',\n",
    "                                         cv = 10)\n",
    "\n",
    "forest_reg_rmse_scores = np.sqrt(-forest_reg_cv_scores)\n",
    "forest_reg_rmse_scores.mean()  \n",
    "#2.558591, so far random forest es the best performer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-tobago",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine Regressor</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "skilled-interest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.08659162080283"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel='linear')  \n",
    "svm_reg.fit(prepared_data, data_labels)\n",
    "svm_cv_scores = cross_val_score(svm_reg, prepared_data, data_labels,\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                cv = 10)\n",
    "svm_rmse_scores = np.sqrt(-svm_cv_scores)\n",
    "svm_rmse_scores.mean()  #Resp.: 3.09, rforest still being the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-murder",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning using GridSearchCV</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "equal-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV  #to fine-tune the hiperparameters of the random f.regressor. This to \n",
    "                                                  # improve the performance of r.f.model.\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True,\n",
    "                           cv=10,\n",
    "                          )\n",
    "\n",
    "grid_search.fit(prepared_data, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eligible-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "noted-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5715192218398215 {'max_features': 2, 'n_estimators': 3}\n",
      "3.1733763358529687 {'max_features': 2, 'n_estimators': 10}\n",
      "2.9582545279429935 {'max_features': 2, 'n_estimators': 30}\n",
      "3.2884828756823277 {'max_features': 4, 'n_estimators': 3}\n",
      "2.9157676481198873 {'max_features': 4, 'n_estimators': 10}\n",
      "2.7541346184672575 {'max_features': 4, 'n_estimators': 30}\n",
      "3.1558401868513934 {'max_features': 6, 'n_estimators': 3}\n",
      "2.818444901591381 {'max_features': 6, 'n_estimators': 10}\n",
      "2.6435815144546435 {'max_features': 6, 'n_estimators': 30}\n",
      "2.8216031890490223 {'max_features': 8, 'n_estimators': 3}\n",
      "2.7937427545550846 {'max_features': 8, 'n_estimators': 10}\n",
      "2.731538156637186 {'max_features': 8, 'n_estimators': 30}\n",
      "3.473805822892421 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "2.822809813277452 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "3.364310680886484 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "2.928852924891443 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "3.0634690881323823 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "2.8006570963563115 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cv_scores = grid_search.cv_results_\n",
    "\n",
    "##printing all the parameters along with their scores\n",
    "for mean_score, params in zip(cv_scores['mean_test_score'], cv_scores[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    #Res.: I am going to choose 2.64 (the lowest value, no 2.55 from above. This is the improved r.forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-mexican",
   "metadata": {},
   "source": [
    "<h3>Checking Feature importance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "serial-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19300645, 0.16976561, 0.16414755, 0.26108427, 0.01822394,\n",
       "       0.12227059, 0.02604562, 0.04004234, 0.00202216, 0.00194882,\n",
       "       0.00144264])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature importances \n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances #ampliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "macro-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acc_on_power', 0.0260456199701643),\n",
       " ('acc_on_cyl', 0.04004234272187999),\n",
       " ('Weight', 0.2610842693240977),\n",
       " ('Model Year', 0.12227059426783451),\n",
       " ('Horsepower', 0.16414754588211078),\n",
       " ('Displacement', 0.1697656104126077),\n",
       " ('Cylinders', 0.19300645425296942),\n",
       " ('Acceleration', 0.018223944070466944)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attrs = [\"acc_on_power\", \"acc_on_cyl\"]\n",
    "numerics = ['float64', 'int64']\n",
    "num_attrs = list(data.select_dtypes(include=numerics))\n",
    "\n",
    "attrs = num_attrs + extra_attrs\n",
    "sorted(zip(attrs, feature_importances), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-question",
   "metadata": {},
   "source": [
    "<h3>Evaluating the entire system on Test Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "attractive-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"MPG\", axis=1)\n",
    "y_test = strat_test_set[\"MPG\"].copy()\n",
    "\n",
    "X_test_preprocessed = preprocess_origin_cols(X_test)\n",
    "X_test_prepared = pipeline_transformer(X_test_preprocessed)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "revolutionary-diary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.079721127346148"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse  #Resp.: 3.0797, we can iterate to improve this result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-myanmar",
   "metadata": {},
   "source": [
    "<h3>Creating a function to cover this entire flow</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mpg(config, model):\n",
    "    \n",
    "    if type(config) == dict:\n",
    "        df = pd.DataFrame(config)\n",
    "    else:\n",
    "        df = config          #By default:true\n",
    "    \n",
    "    preproc_df = preprocess_origin_cols(df)\n",
    "    prepared_df = pipeline_transformer(preproc_df)\n",
    "    #print(prepared_df)\n",
    "    y_pred = model.predict(prepared_df)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "theoretical-amendment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.56      , 17.64666667, 21.64      ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking it on a random sample\n",
    "vehicle_config = {\n",
    "    'Cylinders': [4, 6, 8],\n",
    "    'Displacement': [155.0, 160.0, 165.5],\n",
    "    'Horsepower': [93.0, 130.0, 98.0],\n",
    "    'Weight': [2500.0, 3150.0, 2600.0],\n",
    "    'Acceleration': [15.0, 14.0, 16.0],\n",
    "    'Model Year': [81, 80, 78],\n",
    "    'Origin': [3, 2, 1]\n",
    "}\n",
    "\n",
    "predict_mpg(vehicle_config, final_model) \n",
    "#Res. 3 predictions for 3 rows, it should be close to what I have seen in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-cyprus",
   "metadata": {},
   "source": [
    "<h3>Save the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "virgin-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fossil-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving the model\n",
    "with open(\"model.bin\", 'wb') as f_out:\n",
    "    pickle.dump(final_model, f_out)\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "opposite-memory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.56      , 17.64666667, 21.64      ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##loading the model from the saved file\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    model = pickle.load(f_in)\n",
    "\n",
    "predict_mpg(vehicle_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These results are the same as the predictions in 45th line because of the same vehicle configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "analyzed-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   #install requests packages using cmd, pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:9696/\"  #\"http://0.0.0.0:9696/\" \n",
    "r = requests.post(url, json = vehicle_config)\n",
    "r.text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
